# DataMasking. Маскирование данных с примером для PostgreSQL + ORM Peewee
В данном проекте продемонстрировано применение маскирования персональных данных на примере PostgreSQL с использованием ORM Peewee.
1. Из персональных данных маскирование сделано для: банковских номеров, номеров кредитной карты, ИНН, адресов электронной почты, IP, MAC, имен (ru + eng), паспортных данных, номеров телефона
2. Продемонстрирована дифференциальная приватность, с помощью которой можно маскировать любые некатегориальные значения
# Файлы:
### Основной файл
PostGreTry.py - здесь создается база данных PostgreSQL для демонстрации маскирования. В main() наполняется данными, применяется маскирование каждой колонки, выводится график распределения данных при разных параметрах
### Скрипты
* Banknumber.py - маскирование банковских номеров
* Cardnumber.py - маскирование номеров банковских карт
* EmailMasking.py - маскирование адресов электронной почты
* INNMasking.py - маскирование ИНН
* IPMAC.py - маскирование IP \ MAC адресов
* Names.py - маскирование русских \ иностранных имен
* Passport.py - маскирование паспортных данных
* PhoneNumber.py - маскирование телефонных номеров 
* hash_email.py - Hash-маскирование электронной почты
* DifferencialPrivacy.py - дифференциальная конфиденциальность с помощью механизма Лапласа 
# Теория

#### Маскирование данных
Маскирование данных — это процесс изменения или удаления данных из исходного набора, который может быть применен как к обычным идентификаторам, так и конфиденциальным атрибутам
Цель заключается в том, чтобы преобразовать набор данных T в анонимную версию T′ которая удовлетворяет требованиям и сохраняет как можно больше информации для предполагаемой задачи аналитики данных
#### Обоснованность 
Нельзя просто убрать всю чувствительную информацию из набора данных. Очень важно, чтобы сохранялась полезность для возможности анализа. При работе с данными мы сталкиваемся с проблемой баланса между полезностью данных и рисками их идентификации. Обезличивание всегда приводит к некоторой потере информации и, следовательно, к снижению полезности данных
###### ![image](https://github.com/DanilaAniva/DataMasking/assets/128606792/29ca6f59-e91d-4181-8280-4ebaec08ff66)
### Дифференциальная конфиденциальность 
Механизм дифференциальной конфиденциальности – это функция, которая обеспечивает гарантии потери конфиденциальности путем добавления шума к результату запроса базы данных. Распределение этого шума тщательно откалибровано. Чтобы эта гарантия сохранялась, мы пользуемся чувствительностью и калибровкой распределения. 

Почему не добавить случайный шум? – Это нарушит распределение величин в базе данных и записи станут уязвимыми к атакам злоумышленников. В идеале, если мы убираем или добавляем новые записи к базе данных, распределение в наборе данных должно сохраняться. 

Механизм M есть ϵ –дифференциально конфиденциальный, если:
#### ![image](https://github.com/DanilaAniva/DataMasking/assets/128606792/15ab3878-cbfb-4455-949d-d54291f3227e)
Читается так: вероятность P что злоумышленник может определить на основе ответа M (x) или M(y) ограничено небольшим значением e^ϵ. e^ϵ
Представляет объем информации, полученной злоумышленником, расследующим данные запроса или опубликованный набор данных. В этом сценарии злоумышленник хочет изучить информацию, чтобы определить, находится ли человек в ожидаемом наборе (∈g). Алгоритм M должен удерживать эти границы независимо от того, какая строка была добавлена или удалена

С помощью параметров ДП (дифференциальной приватности) можно задавать, насколько маскированные данные должны быть похожи на исходные. Основные параметры - ε и sensitivity.
К примеру, с ε = 0.5 и sensitivity = 5
#### ![image](https://github.com/DanilaAniva/DataMasking/assets/128606792/1f6dd930-1c43-4ddb-942e-ef3c2d265c57)
Вот ситуация, когда мы выставим больший ε=10 и данные будут более полезными: 
#### ![image](https://github.com/DanilaAniva/DataMasking/assets/128606792/03390fe5-20cf-4a20-b905-26d743c84570)
Так можно балансировать между полезностью данных и конфиденциальностью

Более подробно о методах маскирования можно почитать в #Литература
# Литература
* Hands-On Differential Privacy - Ethan Cowan, Mayana Pereira, Michael Shoemate,  // O'Reilly Media
* The Algorithmic Foundations of Differential Privacy - Aaron Roth, Cynthia Dwork, 2014
* The Architecture of Privacy - Courtney Bowman, Ari Gesher, John K Grant, Daniel Slate, Elissa Lerner,  // O'Reilly Media
* Khaled El Emam, Luk Arbuckle, Anonymizing Health Data // O'Reilly Media, Inc.
